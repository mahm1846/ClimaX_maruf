{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"install/","title":"Installation Guide","text":"clone the repo<pre><code>git clone https://github.com/microsoft/ClimaX\n</code></pre> <code>conda</code><code>docker</code> create and activate env<pre><code>cd ClimaX\nconda env create --file docker/environment.yml\nconda activate ClimaX\n</code></pre> install this package<pre><code># install so the project is in PYTHONPATH\npip install -e .\n</code></pre> build docker container<pre><code>cd docker\ndocker build -t ClimaX .\n</code></pre> run docker container<pre><code>cd ClimaX\ndocker run --gpus all -it --rm --user $(id -u):$(id -g) \\\n-v $(pwd):/code -v /mnt/data:/data --workdir /code -e PYTHONPATH=/code/src \\\nClimaX:latest\n</code></pre> <p>Note</p> <ul> <li><code>--gpus all -it --rm --user $(id -u):$(id -g)</code>: enables using all GPUs and runs an interactive session with current user's UID/GUID to avoid <code>docker</code> writing files as root.</li> <li><code>-v $(pwd):/code -v /mnt/data:/data --workdir /code</code>: mounts current directory and data directory (i.e. the cloned git repo) to <code>/code</code> and <code>/data</code> respectively, and use the <code>code</code> directory as the current working directory.</li> </ul>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#pretraining","title":"Pretraining","text":""},{"location":"usage/#data-preparation","title":"Data Preparation","text":"<p>The code for downloading and preprocessing CMIP6 data is coming soon</p>"},{"location":"usage/#training","title":"Training","text":"<p><pre><code>python src/climax/pretrain/train.py --config &lt;path/to/config&gt;\n</code></pre> For example, to pretrain ClimaX on MPI-ESM dataset on 8 GPUs use <pre><code>python src/climax/pretrain/train.py --config configs/pretrain_climax.yaml \\\n--trainer.strategy=ddp --trainer.devices=8 \\\n--trainer.max_epochs=100 \\\n--data.batch_size=16 \\\n--model.lr=5e-4 --model.beta_1=\"0.9\" --model.beta_2=\"0.95\" \\\n--model.weight_decay=1e-5\n</code></pre></p> <p>Tip</p> <p>Make sure to update the paths of the data directories in the config files (or override them via the CLI).</p>"},{"location":"usage/#pretrained-checkpoints","title":"Pretrained checkpoints","text":"<p>We provide two pretrained checkpoints, one was pretrained on 5.625deg data, and the other was pretrained on 1.40625deg data. Both checkpoints were pretrained using all 5 CMIP6 datasets.</p> <p>Usage: We can load the checkpoint by passing the checkpoint url to the training script. See below for examples.</p>"},{"location":"usage/#global-forecasting","title":"Global Forecasting","text":""},{"location":"usage/#data-preparation_1","title":"Data Preparation","text":"<p>First, download ERA5 data from WeatherBench. The data directory should look like the following <pre><code>5.625deg\n   |-- 10m_u_component_of_wind\n   |-- 10m_v_component_of_wind\n   |-- 2m_temperature\n   |-- constants.nc\n   |-- geopotential\n   |-- relative_humidity\n   |-- specific_humidity\n   |-- temperature\n   |-- toa_incident_solar_radiation\n   |-- total_precipitation\n   |-- u_component_of_wind\n   |-- v_component_of_wind\n</code></pre></p> <p>Then, preprocess the netcdf data into small numpy files and compute important statistics <pre><code>python src/data_preprocessing/nc2np_equally_era5.py \\\n--root_dir /mnt/data/5.625deg \\\n--save_dir /mnt/data/5.625deg_npz \\\n--start_train_year 1979 --start_val_year 2016 \\\n--start_test_year 2017 --end_year 2019 --num_shards 8\n</code></pre></p> <p>The preprocessed data directory will look like the following <pre><code>5.625deg_npz\n   |-- train\n   |-- val\n   |-- test\n   |-- normalize_mean.npz\n   |-- normalize_std.npz\n   |-- lat.npy\n   |-- lon.npy\n</code></pre></p>"},{"location":"usage/#training_1","title":"Training","text":"<p>To finetune ClimaX for global forecasting, use <pre><code>python src/climax/global_forecast/train.py --config &lt;path/to/config&gt;\n</code></pre> For example, to finetune ClimaX on 8 GPUs use <pre><code>python src/climax/global_forecast/train.py --config configs/global_forecast_climax.yaml \\\n--trainer.strategy=ddp --trainer.devices=8 \\\n--trainer.max_epochs=50 \\\n--data.root_dir=/mnt/data/5.625deg_npz \\\n--data.predict_range=72 --data.out_variables=['z_500','t_850','t2m'] \\\n--data.batch_size=16 \\\n--model.pretrained_path='https://climaxrelease.blob.core.windows.net/checkpoints/ClimaX-5.625deg.ckpt' \\\n--model.lr=5e-7 --model.beta_1=\"0.9\" --model.beta_2=\"0.99\" \\\n--model.weight_decay=1e-5\n</code></pre> To train ClimaX from scratch, set <code>--model.pretrained_path=\"\"</code>.</p>"},{"location":"usage/#regional-forecasting","title":"Regional Forecasting","text":""},{"location":"usage/#data-preparation_2","title":"Data Preparation","text":"<p>We use the same ERA5 data as in global forecasting and extract the regional data on the fly during training. If you have already downloaded and preprocessed the data, you do not have to do it again.</p>"},{"location":"usage/#training_2","title":"Training","text":"<p>To finetune ClimaX for regional forecasting, use <pre><code>python src/climax/regional_forecast/train.py --config &lt;path/to/config&gt;\n</code></pre> For example, to finetune ClimaX on North America using 8 GPUs, use <pre><code>python src/climax/regional_forecast/train.py --config configs/regional_forecast_climax.yaml \\\n--trainer.strategy=ddp --trainer.devices=8 \\\n--trainer.max_epochs=50 \\\n--data.root_dir=/mnt/data/5.625deg_npz \\\n--data.region=\"NorthAmerica\"\n--data.predict_range=72 --data.out_variables=['z_500','t_850','t2m'] \\\n--data.batch_size=16 \\\n--model.pretrained_path='https://climaxrelease.blob.core.windows.net/checkpoints/ClimaX-5.625deg.ckpt' \\\n--model.lr=5e-7 --model.beta_1=\"0.9\" --model.beta_2=\"0.99\" \\\n--model.weight_decay=1e-5\n</code></pre> To train ClimaX from scratch, set <code>--model.pretrained_path=\"\"</code>.</p>"},{"location":"usage/#visualization","title":"Visualization","text":"<p>Coming soon</p>"},{"location":"reference/global_forecast/","title":"Global Forecasting","text":""},{"location":"reference/global_forecast/#climax.global_forecast.datamodule.GlobalForecastDataModule","title":"<code>GlobalForecastDataModule</code>","text":"<p>         Bases: <code>LightningDataModule</code></p> <p>DataModule for global forecast data.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>Root directory for sharded data.</p> required <code>variables</code> <code>list</code> <p>List of input variables.</p> required <code>buffer_size</code> <code>int</code> <p>Buffer size for shuffling.</p> required <code>out_variables</code> <code>list</code> <p>List of output variables.</p> <code>None</code> <code>predict_range</code> <code>int</code> <p>Predict range.</p> <code>6</code> <code>hrs_each_step</code> <code>int</code> <p>Hours each step.</p> <code>1</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>64</code> <code>num_workers</code> <code>int</code> <p>Number of workers.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>False</code> Source code in <code>climax/global_forecast/datamodule.py</code> <pre><code>class GlobalForecastDataModule(LightningDataModule):\n\"\"\"DataModule for global forecast data.\n    Args:\n        root_dir (str): Root directory for sharded data.\n        variables (list): List of input variables.\n        buffer_size (int): Buffer size for shuffling.\n        out_variables (list, optional): List of output variables.\n        predict_range (int, optional): Predict range.\n        hrs_each_step (int, optional): Hours each step.\n        batch_size (int, optional): Batch size.\n        num_workers (int, optional): Number of workers.\n        pin_memory (bool, optional): Whether to pin memory.\n    \"\"\"\ndef __init__(\nself,\nroot_dir,\nvariables,\nbuffer_size,\nout_variables=None,\npredict_range: int = 6,\nhrs_each_step: int = 1,\nbatch_size: int = 64,\nnum_workers: int = 0,\npin_memory: bool = False,\n):\nsuper().__init__()\nif num_workers &gt; 1:\nraise NotImplementedError(\n\"num_workers &gt; 1 is not supported yet. Performance will likely degrage too with larger num_workers.\"\n)\n# this line allows to access init params with 'self.hparams' attribute\nself.save_hyperparameters(logger=False)\nif isinstance(out_variables, str):\nout_variables = [out_variables]\nself.hparams.out_variables = out_variables\nself.lister_train = list(dp.iter.FileLister(os.path.join(root_dir, \"train\")))\nself.lister_val = list(dp.iter.FileLister(os.path.join(root_dir, \"val\")))\nself.lister_test = list(dp.iter.FileLister(os.path.join(root_dir, \"test\")))\nself.transforms = self.get_normalize()\nself.output_transforms = self.get_normalize(out_variables)\nself.val_clim = self.get_climatology(\"val\", out_variables)\nself.test_clim = self.get_climatology(\"test\", out_variables)\nself.data_train: Optional[IterableDataset] = None\nself.data_val: Optional[IterableDataset] = None\nself.data_test: Optional[IterableDataset] = None\ndef get_normalize(self, variables=None):\nif variables is None:\nvariables = self.hparams.variables\nnormalize_mean = dict(np.load(os.path.join(self.hparams.root_dir, \"normalize_mean.npz\")))\nmean = []\nfor var in variables:\nif var != \"total_precipitation\":\nmean.append(normalize_mean[var])\nelse:\nmean.append(np.array([0.0]))\nnormalize_mean = np.concatenate(mean)\nnormalize_std = dict(np.load(os.path.join(self.hparams.root_dir, \"normalize_std.npz\")))\nnormalize_std = np.concatenate([normalize_std[var] for var in variables])\nreturn transforms.Normalize(normalize_mean, normalize_std)\ndef get_lat_lon(self):\nlat = np.load(os.path.join(self.hparams.root_dir, \"lat.npy\"))\nlon = np.load(os.path.join(self.hparams.root_dir, \"lon.npy\"))\nreturn lat, lon\ndef get_climatology(self, partition=\"val\", variables=None):\npath = os.path.join(self.hparams.root_dir, partition, \"climatology.npz\")\nclim_dict = np.load(path)\nif variables is None:\nvariables = self.hparams.variables\nclim = np.concatenate([clim_dict[var] for var in variables])\nclim = torch.from_numpy(clim)\nreturn clim\ndef setup(self, stage: Optional[str] = None):\n# load datasets only if they're not loaded already\nif not self.data_train and not self.data_val and not self.data_test:\nself.data_train = ShuffleIterableDataset(\nIndividualForecastDataIter(\nForecast(\nNpyReader(\nfile_list=self.lister_train,\nstart_idx=0,\nend_idx=1,\nvariables=self.hparams.variables,\nout_variables=self.hparams.out_variables,\nshuffle=True,\nmulti_dataset_training=False,\n),\nmax_predict_range=self.hparams.predict_range,\nrandom_lead_time=False,\nhrs_each_step=self.hparams.hrs_each_step,\n),\ntransforms=self.transforms,\noutput_transforms=self.output_transforms,\n),\nbuffer_size=self.hparams.buffer_size,\n)\nself.data_val = IndividualForecastDataIter(\nForecast(\nNpyReader(\nfile_list=self.lister_val,\nstart_idx=0,\nend_idx=1,\nvariables=self.hparams.variables,\nout_variables=self.hparams.out_variables,\nshuffle=False,\nmulti_dataset_training=False,\n),\nmax_predict_range=self.hparams.predict_range,\nrandom_lead_time=False,\nhrs_each_step=self.hparams.hrs_each_step,\n),\ntransforms=self.transforms,\noutput_transforms=self.output_transforms,\n)\nself.data_test = IndividualForecastDataIter(\nForecast(\nNpyReader(\nfile_list=self.lister_test,\nstart_idx=0,\nend_idx=1,\nvariables=self.hparams.variables,\nout_variables=self.hparams.out_variables,\nshuffle=False,\nmulti_dataset_training=False,\n),\nmax_predict_range=self.hparams.predict_range,\nrandom_lead_time=False,\nhrs_each_step=self.hparams.hrs_each_step,\n),\ntransforms=self.transforms,\noutput_transforms=self.output_transforms,\n)\ndef train_dataloader(self):\nreturn DataLoader(\nself.data_train,\nbatch_size=self.hparams.batch_size,\ndrop_last=False,\nnum_workers=self.hparams.num_workers,\npin_memory=self.hparams.pin_memory,\ncollate_fn=collate_fn,\n)\ndef val_dataloader(self):\nreturn DataLoader(\nself.data_val,\nbatch_size=self.hparams.batch_size,\nshuffle=False,\ndrop_last=False,\nnum_workers=self.hparams.num_workers,\npin_memory=self.hparams.pin_memory,\ncollate_fn=collate_fn,\n)\ndef test_dataloader(self):\nreturn DataLoader(\nself.data_test,\nbatch_size=self.hparams.batch_size,\nshuffle=False,\ndrop_last=False,\nnum_workers=self.hparams.num_workers,\npin_memory=self.hparams.pin_memory,\ncollate_fn=collate_fn,\n)\n</code></pre>"},{"location":"reference/global_forecast/#climax.global_forecast.module.GlobalForecastModule","title":"<code>GlobalForecastModule</code>","text":"<p>         Bases: <code>LightningModule</code></p> <p>Lightning module for global forecasting with the ClimaX model.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>ClimaX</code> <p>ClimaX model.</p> required <code>pretrained_path</code> <code>str</code> <p>Path to pre-trained checkpoint.</p> <code>''</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>beta_1</code> <code>float</code> <p>Beta 1 for AdamW.</p> <code>0.9</code> <code>beta_2</code> <code>float</code> <p>Beta 2 for AdamW.</p> <code>0.99</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for AdamW.</p> <code>1e-05</code> <code>warmup_epochs</code> <code>int</code> <p>Number of warmup epochs.</p> <code>10000</code> <code>max_epochs</code> <code>int</code> <p>Number of total epochs.</p> <code>200000</code> <code>warmup_start_lr</code> <code>float</code> <p>Starting learning rate for warmup.</p> <code>1e-08</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-08</code> Source code in <code>climax/global_forecast/module.py</code> <pre><code>class GlobalForecastModule(LightningModule):\n\"\"\"Lightning module for global forecasting with the ClimaX model.\n    Args:\n        net (ClimaX): ClimaX model.\n        pretrained_path (str, optional): Path to pre-trained checkpoint.\n        lr (float, optional): Learning rate.\n        beta_1 (float, optional): Beta 1 for AdamW.\n        beta_2 (float, optional): Beta 2 for AdamW.\n        weight_decay (float, optional): Weight decay for AdamW.\n        warmup_epochs (int, optional): Number of warmup epochs.\n        max_epochs (int, optional): Number of total epochs.\n        warmup_start_lr (float, optional): Starting learning rate for warmup.\n        eta_min (float, optional): Minimum learning rate.\n    \"\"\"\ndef __init__(\nself,\nnet: ClimaX,\npretrained_path: str = \"\",\nlr: float = 5e-4,\nbeta_1: float = 0.9,\nbeta_2: float = 0.99,\nweight_decay: float = 1e-5,\nwarmup_epochs: int = 10000,\nmax_epochs: int = 200000,\nwarmup_start_lr: float = 1e-8,\neta_min: float = 1e-8,\n):\nsuper().__init__()\nself.save_hyperparameters(logger=False, ignore=[\"net\"])\nself.net = net\nif len(pretrained_path) &gt; 0:\nself.load_pretrained_weights(pretrained_path)\ndef load_pretrained_weights(self, pretrained_path):\nif pretrained_path.startswith('http'):\ncheckpoint = torch.hub.load_state_dict_from_url(pretrained_path)\nelse:\ncheckpoint = torch.load(pretrained_path, map_location=torch.device(\"cpu\"))\nprint(\"Loading pre-trained checkpoint from: %s\" % pretrained_path)\ncheckpoint_model = checkpoint[\"state_dict\"]\n# interpolate positional embedding\ninterpolate_pos_embed(self.net, checkpoint_model, new_size=self.net.img_size)\nstate_dict = self.state_dict()\n# checkpoint_keys = list(checkpoint_model.keys())\nfor k in list(checkpoint_model.keys()):\nif \"channel\" in k:\ncheckpoint_model[k.replace(\"channel\", \"var\")] = checkpoint_model[k]\ndel checkpoint_model[k]\nfor k in list(checkpoint_model.keys()):\nif k not in state_dict.keys() or checkpoint_model[k].shape != state_dict[k].shape:\nprint(f\"Removing key {k} from pretrained checkpoint\")\ndel checkpoint_model[k]\n# load pre-trained model\nmsg = self.load_state_dict(checkpoint_model, strict=False)\nprint(msg)\ndef set_denormalization(self, mean, std):\nself.denormalization = transforms.Normalize(mean, std)\ndef set_lat_lon(self, lat, lon):\nself.lat = lat\nself.lon = lon\ndef set_pred_range(self, r):\nself.pred_range = r\ndef set_val_clim(self, clim):\nself.val_clim = clim\ndef set_test_clim(self, clim):\nself.test_clim = clim\ndef training_step(self, batch: Any, batch_idx: int):\nx, y, lead_times, variables, out_variables = batch\nloss_dict, _ = self.net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=self.lat)\nloss_dict = loss_dict[0]\nfor var in loss_dict.keys():\nself.log(\n\"train/\" + var,\nloss_dict[var],\non_step=True,\non_epoch=False,\nprog_bar=True,\n)\nloss = loss_dict[\"loss\"]\nreturn loss\ndef validation_step(self, batch: Any, batch_idx: int):\nx, y, lead_times, variables, out_variables = batch\nif self.pred_range &lt; 24:\nlog_postfix = f\"{self.pred_range}_hours\"\nelse:\ndays = int(self.pred_range / 24)\nlog_postfix = f\"{days}_days\"\nall_loss_dicts = self.net.evaluate(\nx,\ny,\nlead_times,\nvariables,\nout_variables,\ntransform=self.denormalization,\nmetrics=[lat_weighted_mse_val, lat_weighted_rmse, lat_weighted_acc],\nlat=self.lat,\nclim=self.val_clim,\nlog_postfix=log_postfix,\n)\nloss_dict = {}\nfor d in all_loss_dicts:\nfor k in d.keys():\nloss_dict[k] = d[k]\nfor var in loss_dict.keys():\nself.log(\n\"val/\" + var,\nloss_dict[var],\non_step=False,\non_epoch=True,\nprog_bar=False,\nsync_dist=True,\n)\nreturn loss_dict\ndef test_step(self, batch: Any, batch_idx: int):\nx, y, lead_times, variables, out_variables = batch\nif self.pred_range &lt; 24:\nlog_postfix = f\"{self.pred_range}_hours\"\nelse:\ndays = int(self.pred_range / 24)\nlog_postfix = f\"{days}_days\"\nall_loss_dicts = self.net.evaluate(\nx,\ny,\nlead_times,\nvariables,\nout_variables,\ntransform=self.denormalization,\nmetrics=[lat_weighted_mse_val, lat_weighted_rmse, lat_weighted_acc],\nlat=self.lat,\nclim=self.test_clim,\nlog_postfix=log_postfix,\n)\nloss_dict = {}\nfor d in all_loss_dicts:\nfor k in d.keys():\nloss_dict[k] = d[k]\nfor var in loss_dict.keys():\nself.log(\n\"test/\" + var,\nloss_dict[var],\non_step=False,\non_epoch=True,\nprog_bar=False,\nsync_dist=True,\n)\nreturn loss_dict\ndef configure_optimizers(self):\ndecay = []\nno_decay = []\nfor name, m in self.named_parameters():\nif \"var_embed\" in name or \"pos_embed\" in name or \"time_pos_embed\" in name:\nno_decay.append(m)\nelse:\ndecay.append(m)\noptimizer = torch.optim.AdamW(\n[\n{\n\"params\": decay,\n\"lr\": self.hparams.lr,\n\"betas\": (self.hparams.beta_1, self.hparams.beta_2),\n\"weight_decay\": self.hparams.weight_decay,\n},\n{\n\"params\": no_decay,\n\"lr\": self.hparams.lr,\n\"betas\": (self.hparams.beta_1, self.hparams.beta_2),\n\"weight_decay\": 0,\n},\n]\n)\nlr_scheduler = LinearWarmupCosineAnnealingLR(\noptimizer,\nself.hparams.warmup_epochs,\nself.hparams.max_epochs,\nself.hparams.warmup_start_lr,\nself.hparams.eta_min,\n)\nscheduler = {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}\nreturn {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n</code></pre>"},{"location":"reference/pretrain/","title":"Pretraining","text":""},{"location":"reference/pretrain/#climax.pretrain.datamodule.MultiSourceDataModule","title":"<code>MultiSourceDataModule</code>","text":"<p>         Bases: <code>LightningDataModule</code></p> <p>DataModule for multi-source data.</p> <p>Parameters:</p> Name Type Description Default <code>dict_root_dirs</code> <code>Dict</code> <p>Dictionary of root directories for each source.</p> required <code>dict_start_idx</code> <code>Dict</code> <p>Dictionary of start indices ratio (between 0.0 and 1.0) for each source.</p> required <code>dict_end_idx</code> <code>Dict</code> <p>Dictionary of end indices ratio (between 0.0 and 1.0) for each source.</p> required <code>dict_buffer_sizes</code> <code>Dict</code> <p>Dictionary of buffer sizes for each source.</p> required <code>dict_in_variables</code> <code>Dict</code> <p>Dictionary of input variables for each source.</p> required <code>dict_out_variables</code> <code>Dict</code> <p>Dictionary of output variables for each source.</p> required <code>dict_max_predict_ranges</code> <code>Dict</code> <p>Dictionary of maximum predict ranges for each source.</p> <code>{'mpi-esm': 28}</code> <code>dict_random_lead_time</code> <code>Dict</code> <p>Dictionary of whether to use random lead time for each source.</p> <code>{'mpi-esm': True}</code> <code>dict_hrs_each_step</code> <code>Dict</code> <p>Dictionary of hours each step for each source.</p> <code>{'mpi-esm': 6}</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>64</code> <code>num_workers</code> <code>int</code> <p>Number of workers.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>False</code> Source code in <code>climax/pretrain/datamodule.py</code> <pre><code>class MultiSourceDataModule(LightningDataModule):\n\"\"\"DataModule for multi-source data.\n    Args:\n        dict_root_dirs (Dict): Dictionary of root directories for each source.\n        dict_start_idx (Dict): Dictionary of start indices ratio (between 0.0 and 1.0) for each source.\n        dict_end_idx (Dict): Dictionary of end indices ratio (between 0.0 and 1.0) for each source.\n        dict_buffer_sizes (Dict): Dictionary of buffer sizes for each source.\n        dict_in_variables (Dict): Dictionary of input variables for each source.\n        dict_out_variables (Dict): Dictionary of output variables for each source.\n        dict_max_predict_ranges (Dict, optional): Dictionary of maximum predict ranges for each source.\n        dict_random_lead_time (Dict, optional): Dictionary of whether to use random lead time for each source.\n        dict_hrs_each_step (Dict, optional): Dictionary of hours each step for each source.\n        batch_size (int, optional): Batch size.\n        num_workers (int, optional): Number of workers.\n        pin_memory (bool, optional): Whether to pin memory.\n    \"\"\"\ndef __init__(\nself,\ndict_root_dirs: Dict,\ndict_start_idx: Dict,\ndict_end_idx: Dict,\ndict_buffer_sizes: Dict,\ndict_in_variables: Dict,\ndict_out_variables: Dict,\ndict_max_predict_ranges: Dict = {\"mpi-esm\": 28},\ndict_random_lead_time: Dict = {\"mpi-esm\": True},\ndict_hrs_each_step: Dict = {\"mpi-esm\": 6},\nbatch_size: int = 64,\nnum_workers: int = 0,\npin_memory: bool = False,\n):\nsuper().__init__()\nif num_workers &gt; 1:\nraise NotImplementedError(\n\"num_workers &gt; 1 is not supported yet. Performance will likely degrage too with larger num_workers.\"\n)\n# this line allows to access init params with 'self.hparams' attribute\nself.save_hyperparameters(logger=False)\nout_variables = {}\nfor k, list_out in dict_out_variables.items():\nif list_out is not None:\nout_variables[k] = list_out\nelse:\nout_variables[k] = dict_in_variables[k]\nself.hparams.dict_out_variables = out_variables\nself.dict_lister_trains = {\nk: list(dp.iter.FileLister(os.path.join(root_dir, \"train\"))) for k, root_dir in dict_root_dirs.items()\n}\nself.train_dataset_args = {\nk: {\n\"max_predict_range\": dict_max_predict_ranges[k],\n\"random_lead_time\": dict_random_lead_time[k],\n\"hrs_each_step\": dict_hrs_each_step[k],\n}\nfor k in dict_root_dirs.keys()\n}\nself.transforms = self.get_normalize()\nself.output_transforms = self.get_normalize(self.hparams.dict_out_variables)\nself.dict_data_train: Optional[Dict] = None\ndef get_normalize(self, dict_variables: Optional[Dict] = None):\nif dict_variables is None:\ndict_variables = self.hparams.dict_in_variables\ndict_transforms = {}\nfor k in dict_variables.keys():\nroot_dir = self.hparams.dict_root_dirs[k]\nvariables = dict_variables[k]\nnormalize_mean = dict(np.load(os.path.join(root_dir, \"normalize_mean.npz\")))\nmean = []\nfor var in variables:\nif var != \"total_precipitation\":\nmean.append(normalize_mean[var])\nelse:\nmean.append(np.array([0.0]))\nnormalize_mean = np.concatenate(mean)\nnormalize_std = dict(np.load(os.path.join(root_dir, \"normalize_std.npz\")))\nnormalize_std = np.concatenate([normalize_std[var] for var in variables])\ndict_transforms[k] = transforms.Normalize(normalize_mean, normalize_std)\nreturn dict_transforms\ndef get_lat_lon(self):\n# assume different data sources have the same lat and lon coverage\nlat = np.load(os.path.join(list(self.hparams.dict_root_dirs.values())[0], \"lat.npy\"))\nlon = np.load(os.path.join(list(self.hparams.dict_root_dirs.values())[0], \"lon.npy\"))\nreturn lat, lon\ndef setup(self, stage: Optional[str] = None):\n# load datasets only if they're not loaded already\nif not self.dict_data_train:\ndict_data_train = {}\nfor k in self.dict_lister_trains.keys():\nlister_train = self.dict_lister_trains[k]\nstart_idx = self.hparams.dict_start_idx[k]\nend_idx = self.hparams.dict_end_idx[k]\nvariables = self.hparams.dict_in_variables[k]\nout_variables = self.hparams.dict_out_variables[k]\nmax_predict_range = self.hparams.dict_max_predict_ranges[k]\nrandom_lead_time = self.hparams.dict_random_lead_time[k]\nhrs_each_step = self.hparams.dict_hrs_each_step[k]\ntransforms = self.transforms[k]\noutput_transforms = self.output_transforms[k]\nbuffer_size = self.hparams.dict_buffer_sizes[k]\ndict_data_train[k] = ShuffleIterableDataset(\nIndividualForecastDataIter(\nForecast(\nNpyReader(\nlister_train,\nstart_idx=start_idx,\nend_idx=end_idx,\nvariables=variables,\nout_variables=out_variables,\nshuffle=True,\nmulti_dataset_training=True,\n),\nmax_predict_range=max_predict_range,\nrandom_lead_time=random_lead_time,\nhrs_each_step=hrs_each_step,\n),\ntransforms,\noutput_transforms,\n),\nbuffer_size,\n)\nself.dict_data_train = dict_data_train\ndef train_dataloader(self):\nif not torch.distributed.is_initialized():\nraise NotImplementedError(\"Only support distributed training\")\nelse:\nnode_rank = int(os.environ[\"NODE_RANK\"])\n# assert that number of datasets is the same as number of nodes\nnum_nodes = os.environ.get(\"NODES\", None)\nif num_nodes is not None:\nnum_nodes = int(num_nodes)\nassert num_nodes == len(self.dict_data_train.keys())\nfor idx, k in enumerate(self.dict_data_train.keys()):\nif idx == node_rank:\ndata_train = self.dict_data_train[k]\nbreak\n# This assumes that the number of datapoints are going to be the same for all datasets\nreturn DataLoader(\ndata_train,\nbatch_size=self.hparams.batch_size,\ndrop_last=True,\nnum_workers=self.hparams.num_workers,\npin_memory=self.hparams.pin_memory,\ncollate_fn=collate_fn,\n)\n</code></pre>"},{"location":"reference/pretrain/#climax.pretrain.module.PretrainModule","title":"<code>PretrainModule</code>","text":"<p>         Bases: <code>LightningModule</code></p> <p>Lightning module for pretraining the ClimaX model.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>ClimaX</code> <p>ClimaX model.</p> required <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>beta_1</code> <code>float</code> <p>Beta 1 for AdamW.</p> <code>0.9</code> <code>beta_2</code> <code>float</code> <p>Beta 2 for AdamW.</p> <code>0.95</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for AdamW.</p> <code>1e-05</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps.</p> <code>10000</code> <code>max_steps</code> <code>int</code> <p>Number of total steps.</p> <code>200000</code> <code>warmup_start_lr</code> <code>float</code> <p>Starting learning rate for warmup.</p> <code>1e-08</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-08</code> Source code in <code>climax/pretrain/module.py</code> <pre><code>class PretrainModule(LightningModule):\n\"\"\"Lightning module for pretraining the ClimaX model.\n    Args:\n        net (ClimaX): ClimaX model.\n        lr (float, optional): Learning rate.\n        beta_1 (float, optional): Beta 1 for AdamW.\n        beta_2 (float, optional): Beta 2 for AdamW.\n        weight_decay (float, optional): Weight decay for AdamW.\n        warmup_steps (int, optional): Number of warmup steps.\n        max_steps (int, optional): Number of total steps.\n        warmup_start_lr (float, optional): Starting learning rate for warmup.\n        eta_min (float, optional): Minimum learning rate.\n    \"\"\"\ndef __init__(\nself,\nnet: ClimaX,\nlr: float = 5e-4,\nbeta_1: float = 0.9,\nbeta_2: float = 0.95,\nweight_decay: float = 1e-5,\nwarmup_steps: int = 10000,\nmax_steps: int = 200000,\nwarmup_start_lr: float = 1e-8,\neta_min: float = 1e-8,\n):\nsuper().__init__()\nself.save_hyperparameters(logger=False, ignore=[\"net\"])\nself.net = net\ndef set_lat_lon(self, lat, lon):\nself.lat = lat\nself.lon = lon\ndef training_step(self, batch: Any, batch_idx: int):\nx, y, lead_times, variables, out_variables = batch\nloss_dict, _ = self.net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=self.lat)\nloss_dict = loss_dict[0]\nfor var in loss_dict.keys():\nself.log(\n\"train/\" + var,\nloss_dict[var],\non_step=True,\non_epoch=False,\nprog_bar=True,\n)\nloss = loss_dict[\"loss\"]\nreturn loss\ndef configure_optimizers(self):\ndecay = []\nno_decay = []\nfor name, m in self.named_parameters():\nif \"var_embed\" in name or \"pos_embed\" in name or \"time_pos_embed\" in name:\nno_decay.append(m)\nelse:\ndecay.append(m)\noptimizer = torch.optim.AdamW(\n[\n{\n\"params\": decay,\n\"lr\": self.hparams.lr,\n\"betas\": (self.hparams.beta_1, self.hparams.beta_2),\n\"weight_decay\": self.hparams.weight_decay,\n},\n{\n\"params\": no_decay,\n\"lr\": self.hparams.lr,\n\"betas\": (self.hparams.beta_1, self.hparams.beta_2),\n\"weight_decay\": 0,\n},\n]\n)\nlr_scheduler = LinearWarmupCosineAnnealingLR(\noptimizer,\nself.hparams.warmup_steps,\nself.hparams.max_steps,\nself.hparams.warmup_start_lr,\nself.hparams.eta_min,\n)\nscheduler = {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}\nreturn {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n</code></pre>"}]}